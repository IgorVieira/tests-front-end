# CrawlerPosts with backbone :computer: 
[![Build Status](https://travis-ci.org/IgorVieira/crawlerPosts.svg?branch=master)](https://travis-ci.org/IgorVieira/crawlerPosts)

Look README in [portuguese](https://github.com/IgorVieira/crawlerPosts/blob/master/Portuguese.md)

## Requirements

- [Nodejs](https://nodejs.org/en/)
- Gulp (npm install gulp -g)
- Nodemon (npm install nodemon -g)
- babel-node (npm install babel-node -g )
- [Mongodb](https://www.mongodb.com/)
- OS (Linux or Mac)  


## Usage

```
 git clone https://github.com/IgorVieira/crawlerPosts.git
```
 
Enter the client directory and:

```
 npm install && gulp


```


In another terminal and in the same directory:
```
npm start

````

In other terminal, enter the server directory and:

```
 npm install
 npm start
 
```

Open in localhost:3000

# Exemplo

For this example I used the site of the jovemnerd to be able to get the main subjects of nerdcasts.

## First

I went to the site and inspected the elements to get the page data.

![JovemNerd](https://github.com/IgorVieira/crawlerPosts/blob/master/images/foto1.png?raw=true)

##Second

I took the elements that contained the data and the url.

![JovemNerd](https://github.com/IgorVieira/crawlerPosts/blob/master/images/foto2.png?raw=true)
Then I added them all to my data mining, and I saved all the data on the page to my database.

![JovemNerd](https://github.com/IgorVieira/crawlerPosts/blob/master/images/foto3.png?raw=true)


# Thanks! =]






